# 命名实体识别(NER)
```
当前单词(实体)的特征：
1. bag-of-words特征：当前词、前后词、前前后后词的英文单词
2. 词性特征：当前词、前后词、前前后后词的单词词性(n. v. adj.)
3. 前缀后缀特征：当前词、前后词、前前后后词的单词的前缀后缀
4. 当前词特征：词长、大写字母个数、是否大写开头、是否包含-、是否包含数字
5. stemming之后加前面的特征

feature Encoding：
当前词可能是Loc属性，在编码的第三位置1，其他为0，即[0,0,1,0,0,0]
常见特征种类：
1. 分类型：男(0,1)、女(1,0)
2. 连续型：身高，可以直接用，也可以以[150-160][160-170][170-180][180-190]转换为离散的，然后独热编码
3. ordinal feature：比如成绩分数的ABCDE等级，可以直接做特征用，也可以转换成分类型，然后把A改成[1,0,0,0,0]

知识图谱由三元组构成：
例句文本：张三毕业于北京大学，目前在北京工作。
抽取出的实体有张三、北京大学、北京
两两的关系为(张三，工作在，北京),(张三，毕业于，北京大学)

关系抽取的方式：
1. 基于规则：抽取"is a"关系，人工定义规则集X(is a)Y,Y(such as)X,Y including X,Y especially X,X or other Y，然后从文本匹配出满足规则的XY，得到两个满足这个关系的实体
2. 监督学习：特征工程：(1)bag-of-word feature：句子中实体及前后的单词(2)词性特征：名词还是动词(3)实体类别特征：ORG PER(4)stemming之后重复上面的特征(5)位置相关信息：两个实体间有多少个单词，这句话在文本中的位置(6)句法分析相关特征：画出句子的句法树P VP S，找两个节点的最短距离(7)依存文法相关：最短路径。分类模型：(1)假定两个实体之间只有k种可能分类，构造一个分类器SVM 神经网络 GBDT分成K+1个类(还有一个是无分类，两个实体间无关系)。(2)又或者先套一个二分类判断有没有两个实体关系，有的话再用K分类判断两者之间什么关系。
3. 半监督、无监督学习
3.1 Bootstrap：(原始)：人工定义几个seed(对应关系)，然后去文本中根据seed中的实体形成符合条件的规则，然后把这些规则再应用到文本中，抽取出符合规则的实体添加到原seed中，然后继续形成新的规则，继续找新的tuple(见下图1)。(snowball)：在原始bootstrap的生成规则，生成tuple的基础上，后面增加了评估规则的准确度+过滤掉不好的规则，和评估tuple的准确度+过滤掉不好的tuple。以这4步为一个整体然后循环这4步。在原有的tuple的基础上，根据文档里匹配出的实体五元组，将左中右分别转换成向量形式记为规则，可以计算规则的相似度(合并相似度高的规则)，在检索新的tuple时计算文档中的内容和规则的相似度，把大于0.7的加入的tuple中。
3.2 Distant Supervision
3.3 无监督学习
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/bootstrap-%E5%8E%9F%E5%A7%8B.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/bootstrap-snowball.jpg)

# RNN
```
中间的运算核心只有一个，初始化个W，每次进一个值x，得到一个O和W，然后拿W和下一个x做下一轮运算
可以是任意长度的输入，O有的可以不用，但如果句子太长，中间运算核心乘积小于1，那么就会产生梯度消失，改进方法就是LSTM
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/RNN.jpg)

# CNN
```
先用word2vec这种把每个单词表示成一个向量，图中每个单词表示成了6维向量。
然后用自定义的卷积核，红色的就是2*6的卷积核，一定是6，要包含完整的单词向量，然后计算两两bi-gram之间的卷积，当只有一个向量时用0添加(叫做padding)，最后得到一个9*1的向量。
同理，使用一个3*6的卷积核，计算每三个向量tri-gram(关注3个单词之间的联系)之间的卷积，同样的到一个9*1的向量。
然后将许多个9*1的向量进行，最大池化max-pooling，选出每个向量中的最大值作为最后结果中的一位因为上一步产生了4个9*1的向量，每个向量最大池化，最后就是4*1的向量了。
最后根据需要分类的数量设定最后的输出结果数量，两者之间使用全连接+softmax
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/CNN.jpg)

# Transformer
```
左图Transformer：
输入embedding向量，加上一个这个单词在句中的位置信息(假设有9个单词，每个单词输入向量有300维，根据这个单词是句子中的第几个单词，就给这个单词的向量各个位加几，官方加的数字使用sin cos计算出来的)，然后进入一个块，N一般等于6，相等于串联6个块。
Multi-Head Attention：就是一个self-attention，得到提取出来的特征
Add：把输入向量和attention提取出来的信息进行求和
Norm：把求和的结果进行归一化
Feed Forword：前馈神经网络，就是全连接层
Add & Norm：然后再将原向量和全接连的结果向量相加，均一化

右图self-attention：
同一个输入分成三个箭头分别是QKV，然后QKV分别乘上各自需要学习的参数，得到三个不同的输入
MatMul：乘积，矩阵对应位的乘积，点乘
Scale及后面：归一化，矩阵每位除以矩阵的维度，再加个softmax(分母是每个分子的和)
self-attention就是自己和自己乘
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Transformer.jpg)

# Entity Disambiguation(实体消歧)
```
实体消歧本质是一个词有很多个意思，在不同的上下文中表示的含义不一样。
Q：...今天苹果发布了新手机...
document1：苹果：一种水果...
document2：苹果：科技公司...
在问题中取苹果前面10到20个单词，后面10到20个单词，以及它所有的可能解释组成大文本，分别计算Q，document1，document2的tf-idf，然后计算Q和document1的cos相似度一起Q和document2的cos相似度，认为相似度高的为这个单词在这个句子中的语义。
```

# Entity Resolution(实体统一)
```
给定两个实体，判断是否指向同一个实体。类似于一个人有多个账号，怎么判断这些账号(依据账号的特征信息)属于同一个人。
法一：两个实体是字符串，计算str1和str2的相似度，可以使用编辑距离。
法二：基于规则，类似stemming操作，把"百度有限公司","百度科技公司"等重定义为"百度"，证明是一个实体。
法三：有监督的，假定已经知道有两个实体其实属于一个实体，分别取这两个实体的前后文分别计算tf-idf得到v1和v2，然后又可以concat[v1,v2]做特征再接个分类器判断01，或者直接计算v1和v2的cos相似度的数值作为特征，逻辑回归判断01.

基于图的实体统一：graph上有两个点，怎么判断AB属于同一实体，构造A的特征(age,salary,job(个体特征),...,3(邻居个数),s1,s2,s3(都有哪些邻居，关系特征)...)，构造B的特征(age,salary,job(个体特征),...,3(邻居个数),s1,s2,s3(都有哪些邻居，关系特征)...)，然后计算特征相似度，定个阈值，超过了就merge，否则就认为是两个实体。
```

# Co-reference Resolution(指代消解)
```
张三没有去上班，因为生病了。昨天，李四陪他(A)去了医院。好消息是他(B)已经好了。
这里AB分别指代哪个实体？
最简单的方法是判断距离，离哪个实体近就认为指代哪个实体。
或者是用监督学习，样本(张三,A)=1,(李四,A)=0,(张三,B)=1,(李四,B)=0，然后利用张三左边的单词，张三和A之间的单词，以及A右边的单词，转换成(张三,A)的向量表示，继而训练分类器。
```

# Parsing(句法分析)
```
画句法树(syntax tree)：Microsoft is located in Redmond.
          S
      /       \
   NP          VP
   |        /     \
   N      V         VP
   |      |      /     \
Microsoft is   V        PP
               |       /  \
             located  P     N
                      |     |
                      in Redmond
P：phrase N：名词 V：动词 NP：名词短语 P：介词 PP：介词短语
两个叶子节点间的最短路径可以当作特征。
传统及其==机器翻译是把中文句子利用句法分析画成句法树，然后语言学家把中文句法树转换成英文句法树，再反向利用句法分析把英文句法树转换成英文句子。
现代机器翻译直接把输入的中文句子利用seq2seq的结构就转换成了英文句子了。

CFG(context free grammars) to PCFG(Probabilistic)：
S -> NP VP 1
VP -> V NP 0.7
VP -> V NP PP 0.3
NP -> NP NP 0.2
NP -> NP PP 0.3
NP -> N 0.4
NP -> e 0.1
PP -> P NP 1
即句法树的子节点的种类存在可能性
给定一个句子，生成所有可能的语法树，通过前面的概率，可以得到这个树每一步的概率，将所有概率乘在一起得到这棵树的分数，取分数最高的树作为该句子利用PCFG拆分出的句法树。
但是生成所有可能语法树太慢了，使用动态规划(每次把一个大问题拆成两个小问题)的CKY算法(该算法规定最多是两个输出)。
利用CNF算法将上面的NP->e NP->N VP->V NP PP这种的转换成一个转换成两个的这样的形式。
CKY算法从对角线开始，先只考虑一个单词，列出所有可能生成这个单词的路径和概率，然后构造两个单词的生成可能性(先横再竖)，直到推出右上角，找出概率最高的那个生成路径，就得到了句法树。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/CKY.jpg)