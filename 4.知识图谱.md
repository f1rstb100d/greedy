# 命名实体识别(NER)
```
当前单词(实体)的特征：
1. bag-of-words特征：当前词、前后词、前前后后词的英文单词
2. 词性特征：当前词、前后词、前前后后词的单词词性(n. v. adj.)
3. 前缀后缀特征：当前词、前后词、前前后后词的单词的前缀后缀
4. 当前词特征：词长、大写字母个数、是否大写开头、是否包含-、是否包含数字
5. stemming之后加前面的特征

feature Encoding：
当前词可能是Loc属性，在编码的第三位置1，其他为0，即[0,0,1,0,0,0]
常见特征种类：
1. 分类型：男(0,1)、女(1,0)
2. 连续型：身高，可以直接用，也可以以[150-160][160-170][170-180][180-190]转换为离散的，然后独热编码
3. ordinal feature：比如成绩分数的ABCDE等级，可以直接做特征用，也可以转换成分类型，然后把A改成[1,0,0,0,0]

知识图谱由三元组构成：
例句文本：张三毕业于北京大学，目前在北京工作。
抽取出的实体有张三、北京大学、北京
两两的关系为(张三，工作在，北京),(张三，毕业于，北京大学)

关系抽取的方式：
1. 基于规则：抽取"is a"关系，人工定义规则集X(is a)Y,Y(such as)X,Y including X,Y especially X,X or other Y，然后从文本匹配出满足规则的XY，得到两个满足这个关系的实体
2. 监督学习：特征工程：(1)bag-of-word feature：句子中实体及前后的单词(2)词性特征：名词还是动词(3)实体类别特征：ORG PER(4)stemming之后重复上面的特征(5)位置相关信息：两个实体间有多少个单词，这句话在文本中的位置(6)句法分析相关特征：画出句子的句法树P VP S，找两个节点的最短距离(7)依存文法相关：最短路径。分类模型：(1)假定两个实体之间只有k种可能分类，构造一个分类器SVM 神经网络 GBDT分成K+1个类(还有一个是无分类，两个实体间无关系)。(2)又或者先套一个二分类判断有没有两个实体关系，有的话再用K分类判断两者之间什么关系。
3. 半监督、无监督学习
3.1 Bootstrap
3.2 Distant Supervision
3.3 无监督学习
```

# RNN
```
中间的运算核心只有一个，初始化个W，每次进一个值x，得到一个O和W，然后拿W和下一个x做下一轮运算
可以是任意长度的输入，O有的可以不用，但如果句子太长，中间运算核心乘积小于1，那么就会产生梯度消失，改进方法就是LSTM
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/RNN.jpg)

# CNN
```
先用word2vec这种把每个单词表示成一个向量，图中每个单词表示成了6维向量。
然后用自定义的卷积核，红色的就是2*6的卷积核，一定是6，要包含完整的单词向量，然后计算两两bi-gram之间的卷积，当只有一个向量时用0添加(叫做padding)，最后得到一个9*1的向量。
同理，使用一个3*6的卷积核，计算每三个向量tri-gram(关注3个单词之间的联系)之间的卷积，同样的到一个9*1的向量。
然后将许多个9*1的向量进行，最大池化max-pooling，选出每个向量中的最大值作为最后结果中的一位因为上一步产生了4个9*1的向量，每个向量最大池化，最后就是4*1的向量了。
最后根据需要分类的数量设定最后的输出结果数量，两者之间使用全连接+softmax
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/CNN.jpg)

# Transformer
```
左图Transformer：
输入embedding向量，加上一个这个单词在句中的位置信息(假设有9个单词，每个单词输入向量有300维，根据这个单词是句子中的第几个单词，就给这个单词的向量各个位加几，官方加的数字使用sin cos计算出来的)，然后进入一个块，N一般等于6，相等于串联6个块。
Multi-Head Attention：就是一个self-attention，得到提取出来的特征
Add：把输入向量和attention提取出来的信息进行求和
Norm：把求和的结果进行归一化
Feed Forword：前馈神经网络，就是全连接层
Add & Norm：然后再将原向量和全接连的结果向量相加，均一化

右图self-attention：
同一个输入分成三个箭头分别是QKV，然后QKV分别乘上各自需要学习的参数，得到三个不同的输入
MatMul：乘积，矩阵对应位的乘积，点乘
Scale及后面：归一化，矩阵每位除以矩阵的维度，再加个softmax(分母是每个分子的和)
self-attention就是自己和自己乘
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Transformer.jpg)