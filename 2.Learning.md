# Two branches of Learning
```
1. 专家系统: 人工定义规则，if else
2. 基于概率的系统： 数据D={X,y}, 学习映射f:X->y
```
# Forword Chaining Algorithm and Backward Chaining Algorithm
```
例：if A and C then F
if A and E then G
if B then E
if G then D
求证 if A and B then D
正向的话要依据条件AB按条件顺序多次循环，看新得到什么知识，加到AB的已知里，再去循环，直到得到知识D
反向的话就是已知D，去条件里找想得到D需要什么条件，然后再去找想满足这个条件的前提还需要满足什么条件，直到推出需要满足条件AB
```

# Supervised Learning Algorithms
```
1. 线性回归(Linear Regression)
2. 逻辑回归(Logistic Regression)
3. 朴素贝叶斯(Naive Bayes)
4. 神经网络(Neural Network)
5. SVM(Support Vector Machine)
6. 随机森林(Random Forest)
7. Adaboost
8. CNN(Convolutional Neural Network)
```

# Unsupervised Learning Algorithms
```
1. K-means
2. PCA(Principal Component Analysis)
3. ICA(Independent Component Analysis)
4. MF(Matrix Factorization)
5. LSA(Latent Semantic Analysis)
6. LDA(Latent Dirichlet Allocation)
```

# Naive Bayes
```
在垃圾分类任务中，在最后计算p(广告|垃圾)>p(广告|正常)就可判断是不是垃圾邮件，p(广告|垃圾)=垃圾邮件中"广告"词出现的概率，用"广告"词出现的次数除以垃圾邮件中的总的单词数
Bayes Theorem: p(x|y)=p(y|x)p(x)/p(y)
Conditional Independence: p(x,y|z)=p(x|z)p(y|z) x和y是条件独立于z
做预测: p(正常|内容) ? p(垃圾|内容)
=>  p(内容|正常)p(正常) ? p(内容|垃圾)p(垃圾)  p(正常)p(垃圾)是先验概率，即直到所有邮件中正常邮件和垃圾邮件的比例
=>  p(内容|正常)=p(购买,物品,不是,广告|正常)=p(购买|正常)p(物品|正常)p(不是|正常)p(产品|正常)
例子中用到了:Add-One smoothing, Bayes Theorem, Conditional Independence, 乘积太小转为求log的和
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Naive%20Bayes.jpg)

# Evaluation
|     | Correct  | Not correct  |
|  ----  | ----  | ----  |
| Selected  | TP | FP  |
| Not selected  | FN | TN  |

注: 我预测的为1的是Selected，样本真正标签为1的是Correct
```
1. 准确率
acc = 预测正确的样本数量/总的预测样本数量
当正负样本数量不平均时，不能很好反应模型效果(可能还不如全预测0的准确率高)
2. 精确率(precision)
P = % of selected items that are correct TP/(TP+FP)
3. 召回率(recall)
R = % of correct items that are selected TP/(TP+FN)
P和R是互斥的关系，P高就要牺牲R，R高就要牺牲P
4. F1-score
F1 = 2*P*R/(P+R)
对于正样本可以计算P正 R正 F1正；对于负样本同样可以计算P负 R负 F1负
整体模型的F1就可以取各位的平均，(P正+P负)/2
```

# Logistic Regression
```
P(y|X)是(0,1)之间，而 线性回归wx+b(用来回归预测实际的数值的) 的取值是负无穷到正无穷的，为了把线性回归应用到分类问题上
需要逻辑函数 也就是激活函数sigmoid x的范围是负无穷到正无穷的 y的范围是(0,1) 即y=1/(1+e^(-x))
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0.jpg)
```
对于二分类问题，y的label不是0就是1，可以把这俩的条件概率合并成一个式子
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0%E5%90%88%E5%B9%B6.jpg)
```
判断是否是线性分类器，就是求出决策边界的表达式，也就是y=1和y=0等可能的表达式
所以两式相等然后化简成wx+b=0，所以逻辑回归显然是线性分类器
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%98%AF%E7%BA%BF%E6%80%A7.jpg)
```
根据条件概率，需要最大化目标函数，求使得训练集所有xy的条件概率乘积最大的w和b
log函数是单调的，加个log不影响最后结果
因为p是0点几，一堆p的乘积就会很小，需要再应用对数公式：log(xyz)=logx+logy+logz 将乘转换成加
再加个负号就改为求最小化目标函数
然后将合并的条件概率带入，再应用右上角的对数公式化简
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B01.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B02.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B03.jpg)
```
有了目标函数之后就需要优化算法了，因为需要求最小化目标函数的参数，类似于求极值：
1. 判断是否为凸函数，也就是有几个极值点，取最小的极值点为总的目标函数的最小值
2. 最优化算法：GD(Gradient descent梯度下降法), SGD(Stochastic gradient descent随机梯度下降法), AdaGrad算法, Adam算法
Gradient descent梯度下降法：设置步长，根据每一次的导数偏导数逐步递减参数w
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.jpg)
```
使用梯度下降法在逻辑回归的最小目标函数的求解上
根据第一步的内容，先把sigmoid带入到条件概率中
xi和yi是训练集的一条一条数据，真正的参数是w和b，先对w求偏导(注意复合函数的偏导数)，得到w的更新梯度
再对b求偏导，得到b的更新梯度，带入梯度下降的公式，从初始wb开始，每次遍历所有的i个训练集内容乘上步长然后更新参数

什么时候停止参数更新：
1. 设定个阈值，当t+1次的目标函数的值与第t次的目标函数的值的差值小于阈值，就停止
2. 设定个阈值，当t+1次更新的参数的值与第t次更新的参数的值的差值小于阈值，就停止
3. 每次更新完参数就validation验证集上计算下精确度F1，当发现精确度下降时就early stopping提早停止
4. fixed iteration固定次数迭代，设定100次，可能到第80次参数就不怎么变了，然后重复几次就停止了
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D2.jpg)
```
由于梯度下降法每次需要遍历一遍 i=1到n 个训练条数再更新1次参数，当i太大时影响效率
当samples数量太大时，使用随机梯度下降

随机梯度下降：先shuffle重洗所有训练集，对训练集的每一条内容，仅根据这一条进行一次参数更新，不用去管其他的训练内容，跑完n条训练集也就是更新了n次参数

二者折中法mini-batch梯度下降法：从这n条数据中每次采样出一个小的mini-batch，在这个小的训练集内容上进行梯度下降，即每次跑完mini-batch所有内容进行一次参数更新
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.jpg)

# 有限制条件的优化
```
f(x,y)=x+y   s.t. such that   x^2+y^2=1
使用拉格朗日乘法项，构造
Maximum L=x+y+lamda(x^2+y^2-1)
分别令三个参数的偏导等于0，解出最优解的三个参数
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E6%9C%89%E9%99%90%E5%88%B6%E6%9D%A1%E4%BB%B6%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96.jpg)

# MLE (maximum likelihood estimation) 最大似然估计
```
MLE就是只考虑已观察到的信息，求出想要出现这种信息可能性最大的概率参数
根据已出现的次数，4次正面Head，2次反面Tail，求出抛硬币正反的概率
列出目标函数，也就是如何依据参数得到最大化的观测情况概率，然后对目标函数进行优化求参数
整体逻辑是先确定模型(比如神经网络抽象的)然后模型实例化(比如多少层，每层多少点，最后一层softmax，loss使用cross-entropy)，同时明确目标函数f(theta)，最后来优化目标函数(怎么找最好的theta使得目标函数最大或最小)
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/MLE%E6%9C%80%E4%BC%98%E5%8C%96.jpg)

# 朴素贝叶斯 MLE 目标函数 优化
```
首先定义MLE的已知数据D={(x^1,y^1),(x^2,y^2),...,(x^n,y^n)}
其中x^i是一个document i里面所有的单词，y^i是一个document i的分类(垃圾邮件/正常邮件)
其中x^i=(x1^i,x2^i,...,xmi^i) 其中mi是document i的单词数
例子：假如文档i的句子是x^i=(今天|很|高兴|参加|NLP训练营)，所以mi=5，x1^i=今天，x2^i=很，x3^i=高兴，x4^i=参加，x5^i=NLP训练营

根据MLE和贝叶斯公式，列出maximum P(D)的公式，因为是朴素，所以就是条件独立，即p(xyz|h)=p(x|h)p(y|h)p(z|h)
在图上的公式中i是第i个文档，j是当前文档中第j个单词
取对数之后利用图中b的公式替换a中的部分，其中V是所有文档并集的词典中的各个单词，nij是wj这个单词在第i篇文档中的出现次数，后面举例解释了为什么ab相等

利用对数公式将乘转换成加，因为y^i是文档i的分类标签，增加参数K用来分批每次只考虑一类标签下的文档。利用thetakj和PIk替换图中的变量，其中thetakj是在给定分类k的情况下总的字典第j个单词出现的概率，PIk是类别k出现的概率(根据训练数据可知属于先验概率)，因为每有一篇属于k类的文档就会增加一个logPIk，所以使用nk来表示分类k下的文档数，用乘法替换原来的连加。最后这个就是朴素贝叶斯的目标函数

然后对目标函数进行优化，变量是theta和PI，并且存在限制条件：先验知识中PIk所有类别出现的概率之和应为1，任意类别下所有单词出现概率之和应为1。所以使用拉格朗日乘法项进行优化
对PIk求偏导令其等于0，解得lamda和PIk，PIk即属于第k类的文档数量除以所有总的文档数量，最早的那个朴素贝叶斯例子是用统计的方法直接数的，这次是理论证明为什么

然后再对thetakj求偏导令其等于0，解得lamda和thetakj，thetakj即在第k类所有文档中出现第j个单词的数量和然后除以在第k类所有文档中出现的所有字典单词总数
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC2.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC3.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC4.jpg)

# L1、L2范式
```
当两组数据线性就可以分割的时候，w会趋向于正无穷，在坐标系中也就是一条竖线
解释：w正无穷，则wx+b正无穷，带入到p(y=1|wb)=1/(1+e^(-无穷))趋向于1，p(y=0|wb)=e^(-无穷)/(1+e^(-无穷))趋向于0

想要限制w，从而把w降下来变小，需要使用L2范式：
即在原来的目标函数后面加上lamda倍的L2-norm：argmin-p(yi|xw)+lamda||w||2^2 objective+regularization正则项
lambda是超参数(不属于模型的参数的参数叫超参数)，用来控制L2-norm的大小：lamda=0时无限制，lamda大时w就会变小，lamda小时w就会更大
w是个向量，L2-norm就是向量中每一位的平方之和，即||w||2^2=w1^2+w2^2+...+wn^2

前面提到的利用(随机)梯度下降求目标函数就要在原来的偏导后面加上2*lamda*w(在求对w的偏导的时候)，对b求偏导没影响不用改

过拟合源于：
1. 模型本身选择：逻辑回归简单不容易过拟合，神经网络复杂容易过拟合
2. 模型的参数个数：神经网络layer中unit参数多容易过拟合
3. 模型的参数空间选择：减小参数可以选择的空间(范围)
4. 模型拟合过少的样本：增加训练集防止过拟合

L0-norm：向量w中所有非零元素的个数
L1-norm：向量w的各个元素的绝对值之和，||w||1=|w1|+|w2|+...+|wn|，更容易产生稀疏矩阵即0很多，剩下的非0元素很少但是较大。所以经常把L1-norm用在选择的问题上
L2-norm：向量w的各个元素的平方和，||w||2^2=w1^2+w2^2+...+wn^2，产生的结果时大部分元素非0但是很小
Nuclear-norm核范式：限制得出来的矩阵是low rank的。通过矩阵初等变换把A化为阶梯型矩阵，若该阶梯型矩阵有r个非零行，那A的秩rank(A)就等于r。矩阵的秩度量的就是矩阵的行列之间的相关性。如果矩阵的各行或列是线性无关的，矩阵就是满秩的，也就是秩等于行数。方程组有3个方程，实际上只有2个是有用的，一个是多余的，所以对应的矩阵的秩就是2了。

L1-norm和L2-norm都是为了使theta参数变小，不同的是L1产生了稀疏的解，而L2产生的解非稀疏

ElasticNet：同时结合L1-norm和L2-norm进行正则：即 目标函数+lamda1*||theta||1+lamda2*||theta||2^2

图形解释为什么L1产生的参数中0多，为什么L2产生的参数0少：
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/L1%20L2%20norm%20%E5%9B%BE%E5%83%8F.jpg)