# Two branches of Learning
```
1. 专家系统: 人工定义规则，if else
2. 基于概率的系统： 数据D={X,y}, 学习映射f:X->y
```
# Forword Chaining Algorithm and Backward Chaining Algorithm
```
例：if A and C then F
if A and E then G
if B then E
if G then D
求证 if A and B then D
正向的话要依据条件AB按条件顺序多次循环，看新得到什么知识，加到AB的已知里，再去循环，直到得到知识D
反向的话就是已知D，去条件里找想得到D需要什么条件，然后再去找想满足这个条件的前提还需要满足什么条件，直到推出需要满足条件AB
```

# Supervised Learning Algorithms
```
1. 线性回归(Linear Regression)
2. 逻辑回归(Logistic Regression)
3. 朴素贝叶斯(Naive Bayes)
4. 神经网络(Neural Network)
5. SVM(Support Vector Machine)
6. 随机森林(Random Forest)
7. Adaboost
8. CNN(Convolutional Neural Network)
```

# Unsupervised Learning Algorithms
```
1. K-means
2. PCA(Principal Component Analysis)
3. ICA(Independent Component Analysis)
4. MF(Matrix Factorization)
5. LSA(Latent Semantic Analysis)
6. LDA(Latent Dirichlet Allocation)
```

# Naive Bayes
```
在垃圾分类任务中，在最后计算p(广告|垃圾)>p(广告|正常)就可判断是不是垃圾邮件，p(广告|垃圾)=垃圾邮件中"广告"词出现的概率，用"广告"词出现的次数除以垃圾邮件中的总的单词数
Bayes Theorem: p(x|y)=p(y|x)p(x)/p(y)
Conditional Independence: p(x,y|z)=p(x|z)p(y|z) x和y是条件独立于z
做预测: p(正常|内容) ? p(垃圾|内容)
=>  p(内容|正常)p(正常) ? p(内容|垃圾)p(垃圾)  p(正常)p(垃圾)是先验概率，即直到所有邮件中正常邮件和垃圾邮件的比例
=>  p(内容|正常)=p(购买,物品,不是,广告|正常)=p(购买|正常)p(物品|正常)p(不是|正常)p(产品|正常)
例子中用到了:Add-One smoothing, Bayes Theorem, Conditional Independence, 乘积太小转为求log的和
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Naive%20Bayes.jpg)