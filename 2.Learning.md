# Two branches of Learning
```
1. 专家系统: 人工定义规则，if else
2. 基于概率的系统： 数据D={X,y}, 学习映射f:X->y
```
# Forword Chaining Algorithm and Backward Chaining Algorithm
```
例：if A and C then F
if A and E then G
if B then E
if G then D
求证 if A and B then D
正向的话要依据条件AB按条件顺序多次循环，看新得到什么知识，加到AB的已知里，再去循环，直到得到知识D
反向的话就是已知D，去条件里找想得到D需要什么条件，然后再去找想满足这个条件的前提还需要满足什么条件，直到推出需要满足条件AB
```

# Supervised Learning Algorithms
```
1. 线性回归(Linear Regression)
2. 逻辑回归(Logistic Regression)
3. 朴素贝叶斯(Naive Bayes)
4. 神经网络(Neural Network)
5. SVM(Support Vector Machine)
6. 随机森林(Random Forest)
7. Adaboost
8. CNN(Convolutional Neural Network)
```

# Unsupervised Learning Algorithms
```
1. K-means
2. PCA(Principal Component Analysis)
3. ICA(Independent Component Analysis)
4. MF(Matrix Factorization)
5. LSA(Latent Semantic Analysis)
6. LDA(Latent Dirichlet Allocation)
```

# Naive Bayes
```
在垃圾分类任务中，在最后计算p(广告|垃圾)>p(广告|正常)就可判断是不是垃圾邮件，p(广告|垃圾)=垃圾邮件中"广告"词出现的概率，用"广告"词出现的次数除以垃圾邮件中的总的单词数
Bayes Theorem: p(x|y)=p(y|x)p(x)/p(y)
Conditional Independence: p(x,y|z)=p(x|z)p(y|z) x和y是条件独立于z
做预测: p(正常|内容) ? p(垃圾|内容)
=>  p(内容|正常)p(正常) ? p(内容|垃圾)p(垃圾)  p(正常)p(垃圾)是先验概率，即直到所有邮件中正常邮件和垃圾邮件的比例
=>  p(内容|正常)=p(购买,物品,不是,广告|正常)=p(购买|正常)p(物品|正常)p(不是|正常)p(产品|正常)
例子中用到了:Add-One smoothing, Bayes Theorem, Conditional Independence, 乘积太小转为求log的和
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Naive%20Bayes.jpg)

# Evaluation
|     | Correct  | Not correct  |
|  ----  | ----  | ----  |
| Selected  | TP | FP  |
| Not selected  | FN | TN  |
注: 我预测的为1的是Selected，样本真正标签为1的是Correct
```
1. 准确率
acc = 预测正确的样本数量/总的预测样本数量
当正负样本数量不平均时，不能很好反应模型效果(可能还不如全预测0的准确率高)
2. 精确率(precision)
P = % of selected items that are correct TP/(TP+FP)
3. 召回率(recall)
R = % of correct items that are selected TP/(TP+FN)
P和R是互斥的关系，P高就要牺牲R，R高就要牺牲P
4. F1-score
F1 = 2*P*R/(P+R)
对于正样本可以计算P正 R正 F1正；对于负样本同样可以计算P负 R负 F1负
整体模型的F1就可以取各位的平均，(P正+P负)/2
```

# Logistic Regression
```
P(y|X)是(0,1)之间，而 线性回归wx+b(用来回归预测实际的数值的) 的取值是负无穷到正无穷的，为了把线性回归应用到分类问题上
需要逻辑函数 也就是激活函数sigmoid x的范围是负无穷到正无穷的 y的范围是(0,1) 即y=1/(1+e^(-x))
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0.jpg)
```
对于二分类问题，y的label不是0就是1，可以把这俩的条件概率合并成一个式子
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0%E5%90%88%E5%B9%B6.jpg)
```
判断是否是线性分类器，就是求出决策边界的表达式，也就是y=1和y=0等可能的表达式
所以两式相等然后化简成wx+b=0，所以逻辑回归显然是线性分类器
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%98%AF%E7%BA%BF%E6%80%A7.jpg)
```
根据条件概率，需要最大化目标函数，求使得训练集所有xy的条件概率乘积最大的w和b
log函数是单调的，加个log不影响最后结果
因为p是0点几，一堆p的乘积就会很小，需要再应用对数公式：log(xyz)=logx+logy+logz 将乘转换成加
再加个负号就改为求最小化目标函数
然后将合并的条件概率带入，再应用右上角的对数公式化简
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B01.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B02.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B03.jpg)