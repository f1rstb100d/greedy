# 数据结构
```
哈希表：传统按顺序遍历list是O(N)，把list里面每个值经过哈希运算得到索引编号，当再给定新的元素要去找的时候，先经过那个定义的哈希运算得到了它的索引编号，从而直接从list中拿出来，时间复杂度是O(1)。# Leetcode 242 15
搜索树：二叉树，对于每个节点，节点的左子树只包含小于当前节点的数，其右子树只包含大于当前节点的数。# Leetcode 98 22
堆：堆一定是一个完全二叉树(缺失元素只能是右子树，右下角元素)，最大堆是每个节点都比其子树所有节点大，最小堆是每个节点都比其子树所有节点小。 # Leetcode 347
```

# 交叉验证 cv Cross-Validation
```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

parameters = {'C':[0.001,0.01,0.1,0.5,1,2,5,10]}
lr = LogisticRegression
lr.fit(X_train,y_train).score(X_test,y_test)

clf = GridSearchCV(lr, parameters, cv=5)
clf.fit(X_train,y_train)
clf.score(X_test,y_test)
print(clf.best_params_)
```
```
以cv=5为例，对lamda可选的任何一个值，相当于给定lamda然后计算模型参数w和b，然后把这个模型用验证集计算准确率，重复5次计算一下5折交叉验证的准确率，取平均得到这个lamda的准确率，最后返回出准确率最高的那个超参数值，然后后续训练模型的参数时就可以人工指定这个超参数了
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81.jpg)

# L2正则的作用
```
w1: minimize f(w1)  无正则
w2: minimize f(w2)+lamda||w2||2^2  有正则
w1的参数可选择范围为整个大参数空间，而w2仅被限制到了参数空间的一小部分
所以一定有f(w1)<=f(w2)，即w1更有可能找到更好的参数使得f结果更小
但不一定是越小越好，有可能存在过拟合
添加正则是防止过拟合的一种手段
```

# Grid Search
```
当同时使用L1-norm和L2-norm，就会有两个超参数lamda1和lamda2，两个超参数分别有一个各自的候选集，使用网格搜索遍历所有lamda1和lamda2的组合，使用交叉验证找到最好的(lamda1,lamda2)组合

除了Grid Search，其他一些参数搜索的方法：
1. 随机搜索(Random Search):超参数的可选项是一个范围，随机从范围中选出一个值作为这个超参数，然后把多个超参数组合作为这次交叉验证的参数。可以uniform等概率随机选择，也可以设定一个分布来随机选择超参数
2. 遗传算法(Genetic/Evolutionary Algorithm):若当前lamda值效果挺好的，那么下一个值就继承部分属性(遗传因子)；若效果不好就抛弃这个值
3. 贝叶斯优化(Bayesian Optimization):不断通过观测值调整先验概率，从而调整后验概率
```

# MLE与MAP
```
MLE(最大似然估计):仅通过观测值预测最好的参数theta
MAP(最大后验估计):通过观测值和先验概率去预测最好的参数theta
例：扔不规则硬币，扔了1w次，所以有可能先验概率(别人告我的概率)是不准确的，我可能会完全不相信，就看这1w次正反面得最后的概率
随着样本的增加，MLE比MAP更可信
当样本数据非常多的时候，MAP的解更趋近于MLE的解，因为先验概率在总和的占比原来越小，影响也就越来越小
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/MLE%E4%B8%8EMAP%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F.jpg)
```
假设先验概率为高斯分布正态分布，化简后目标函数后面出现L2-norm
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E9%AB%98%E6%96%AF%E5%85%88%E9%AA%8C%E5%88%B0L2%E6%AD%A3%E5%88%99.jpg)
```
假设先验概率为拉普拉斯分布，化简后目标函数后面出现L1-norm
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%85%88%E9%AA%8C%E5%88%B0L1%E6%AD%A3%E5%88%99.jpg)
```
对于任何模型，加入先验等同于目标函数加入正则
```

# Lasso/Ridge Regression 岭回归 特征筛选
```
样本个数小于数据特征维度，即需要特征选择

枚举法：
为了找到哪些特征更重要，列举出所有的特征组合，计算每种组合最后的accuracy，得到最好的特征组合，也就是最有用的特征

Greedy Approach:每一次都是当前局部的最优解
Forward Stepwise:
假设当前可选的特征为[f1,f2,f3,f4,f5]，通过对f单个循环找到最好的accuracy，假设是f2；剩下的可选特征为[f1,f3,f4,f5]，再通过计算{f1,f2},{f2,f3},{f2,f4},{f2,f5}最好的accuracy，假设是{f2,f4}；剩下的可选特征是[f1,f3,f5]，再计算{f1,f2,f4},{f2,f3,f4},{f2,f4,f5}找到最高的accuracy，发现还没原来的{f2,f4}高，那么停止算法，{f2,f4}就是最好的特征
Backward Stepwise:
假设当前可选的特征为[f1,f2,f3,f4,f5]，每次删掉一个特征计算{f1,f2,f3,f4},{f1,f2,f3,f5},{f1,f2,f4,f5},{f1,f3,f4,f5},{f2,f3,f4,f5}找到最高的accuracy，假设是{f1,f3,f4,f5}，当发现这个值比5个全选的accuracy还高，那么就确定当前的best_feature组合是{f1,f3,f4,f5}。然后再一次删掉一个特征，计算{f1,f3,f4},{f1,f3,f5},{f1,f4,f5},{f3,f4,f5}的accuracy，假设{f1,f3,f4}的accuracy最高，那么就确定当前的best_feature组合是{f1,f3,f4}。然后再依次删掉一个特征，发现每次的accuracy都比{f1,f3,f4}低，那么就可以确定{f1,f3,f4}是最好的特征组合。

via Regularization: 通过正则来筛选特征
Lasso Regression: 线性回归+L1-norm
岭回归：线性回归+L2-norm

但是Lasso回归有一个很大的问题，就是由于L1正则化项用的是绝对值之和，导致损失函数有不可导的点。也就是说，梯度下降法等优化算法对它统统失效了。
```

# Coordinate Descent 坐标下降法
```
坐标下降法是非梯度优化算法。与梯度优化算法沿着梯度最速下降的方向寻找函数最小值不同，坐标下降法依次沿着坐标轴的方向最小化目标函数值。
求f(x1,x2,x3,...,xn)的最小值时，分别求每个参数的偏导复杂，那么坐标下降法就是迭代地通过将大多数自变量固定（即看作已知常量），而只针对剩余的自变量f(x3)求极值的过程。
梯度下降法在图上是该点导数的切线方向，而坐标下降法是横平竖直的线。
假设f(x,y)=5x^2-6xy+5y，求(x,y)使目标函数最小
起始点（-0.5, -1.0），此时f =3.25，固定x，将f看成关于y的一元二次方程并求当f最小时y的值：
f(y|x=-0.5)=5y^2+3y+1.25
f'(y|x=-0.5)=10y+3 令其=0
y=-0.3
即，现在自变量的取值就更新成了（-0.5, -0.3）， f = 0.8
```
![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Coordinate_descent.svg/800px-Coordinate_descent.svg.png)

# Coordinate Descent for LASSO
```
目标函数为线性回归+L1-norm，令i为样本编号，令j为第j个特征
对第l个特征的权重求偏导数，先是复合函数求偏导，然后把全部的j遍历拆成等于l和不等于l的，然后展开乘法，令前面部分为Cl，后面为al成wl，al是平方和所以al大于0

因为L1-norm中wl存在绝对值，所以分三种情况讨论wl大于0 wl小于0 wl等于0。化简了绝对值之后令其偏导等于0，分别解出Cl和lamda之间的关系

不同于梯度下降，坐标下降偏导等于0求出来的值就是更新之后的值(参考上面例子)
所以依据上面三种情况，新的wl也分三类讨论，当满足Cl在中间时，wl的权值更新成0，也就是为什么LASSO之后会有很多特征的权值为0
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/LASSO%E5%9D%90%E6%A0%87%E4%B8%8B%E9%99%8D1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/LASSO%E5%9D%90%E6%A0%87%E4%B8%8B%E9%99%8D2.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/LASSO%E5%9D%90%E6%A0%87%E4%B8%8B%E9%99%8D3.jpg)