# 神经网络
```
神经网络一个节点可以分成两部分：
Pre-activation: 由上一层每一个节点和权值的乘积再最后加个偏置b得到
Post-activation: 将Pre-的结果嵌套个激活函数g()
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Pre-activation%20Post-activation.jpg)

```
激活函数：
线性激活函数：g(a)=a，没什么实际意义，原样输出。假设每一层的激活函数都是线性激活，合并之后其实只有一层。
sigmoid激活函数：g(a)=1/(1+e^(-a))，将任意值映射成一个(0,1)之间的数。
tanh激活函数：公式如图，将任意值映射成一个(-1,1)之间的数。
relu激活函数：g(a)=max(0,a)，把线性激活函数小于0的部分全部定义成0。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Linear%20activation%20function1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Linear%20activation%20function2.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/sigmoid%20activation.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/tanh%20activation.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/relu%20activation.jpg)

```
单层神经网络：除去输入层和输出层，中间只有1层隐藏层的叫单层神经网络。通过输入层x和权值w以及偏置b计算隐藏层每个a(x)的值，然后使用激活函数求出h(x)=g(a(x))，再乘上输出层的权值以及偏置得到输出节点的Pre-activation值，输出层的Post-activation一般如果是分类问题激活函数就会选择softmax。
多层神经网络：除去输入层和输出层，中间有多少层就叫多少层神经网络。同样的对每个隐藏层节点先用上一层结果和权值偏置求Pre-activation a(x)，然后 使用激活函数求Post-activation h(x)，输出层使用softmax作为激活函数求出每种可能的均一化概率，然后根据求出的概率和实际的标签label求两者的交叉熵作为这次结果的loss。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/single%20layer%20neural%20network.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Multi-layer%20neural%20network.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Multiple%20output%20neural%20network.jpg)

```
Universal Approximation Theorem(万能近似定理)：如果有足够的隐藏层单元，那么单层神经网络就可以近似估计任何连续函数。但关键的问题是像SGD,Adagrad这样的优化算法也无法找到最好的网络参数。
```

```
假设θ是1到L+1层的所有权值w和偏置b(中间L层隐藏层，有L层参数，最后输出层可以看作第L+1层，一共就是L+1层参数)，(x,y)是一次训练的输入，y是真实标签，f(x;θ)是预测值，根据损失函数使用SGD优化参数。
多分类的损失函数由交叉熵求出，true lable是one-hot格式，定义I()是indicator函数，只有y=c的时候才是I函数才是1，否则都是0.
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/neural%20network%20loss1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/neural%20network%20loss2.jpg)

```
反向传播：从头跑到尾得到预测值，然后使用交叉熵计算出了损失函数，需要计算损失函数对各个参数的导数，来设定下一次新的模型的参数。整个模型的参数有：输出层的f(x)，输出层的a(x)，隐藏层的h(x)，隐藏层的a(x)，每一层与层之间的w和b。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%971.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%972.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%973.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%974.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%975.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%976.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%977.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%978.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B.jpg)