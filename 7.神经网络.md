# 神经网络
```
神经网络一个节点可以分成两部分：
Pre-activation: 由上一层每一个节点和权值的乘积再最后加个偏置b得到
Post-activation: 将Pre-的结果嵌套个激活函数g()
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Pre-activation%20Post-activation.jpg)

```
激活函数：
线性激活函数：g(a)=a，没什么实际意义，原样输出。假设每一层的激活函数都是线性激活，合并之后其实只有一层。
sigmoid激活函数：g(a)=1/(1+e^(-a))，将任意值映射成一个(0,1)之间的数。
tanh激活函数：公式如图，将任意值映射成一个(-1,1)之间的数。
relu激活函数：g(a)=max(0,a)，把线性激活函数小于0的部分全部定义成0。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Linear%20activation%20function1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Linear%20activation%20function2.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/sigmoid%20activation.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/tanh%20activation.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/relu%20activation.jpg)

```
单层神经网络：除去输入层和输出层，中间只有1层隐藏层的叫单层神经网络。通过输入层x和权值w以及偏置b计算隐藏层每个a(x)的值，然后使用激活函数求出h(x)=g(a(x))，再乘上输出层的权值以及偏置得到输出节点的Pre-activation值，输出层的Post-activation一般如果是分类问题激活函数就会选择softmax。
多层神经网络：除去输入层和输出层，中间有多少层就叫多少层神经网络。同样的对每个隐藏层节点先用上一层结果和权值偏置求Pre-activation a(x)，然后 使用激活函数求Post-activation h(x)，输出层使用softmax作为激活函数求出每种可能的均一化概率，然后根据求出的概率和实际的标签label求两者的交叉熵作为这次结果的loss。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/single%20layer%20neural%20network.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Multi-layer%20neural%20network.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Multiple%20output%20neural%20network.jpg)

```
Universal Approximation Theorem(万能近似定理)：如果有足够的隐藏层单元，那么单层神经网络就可以近似估计任何连续函数。但关键的问题是像SGD,Adagrad这样的优化算法也无法找到最好的网络参数。
```

```
假设θ是1到L+1层的所有权值w和偏置b(中间L层隐藏层，有L层参数，最后输出层可以看作第L+1层，一共就是L+1层参数)，(x,y)是一次训练的输入，y是真实标签，f(x;θ)是预测值，根据损失函数使用SGD优化参数。
多分类的损失函数由交叉熵求出，true lable是one-hot格式，定义I()是indicator函数，只有y=c的时候才是I函数才是1，否则都是0.
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/neural%20network%20loss1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/neural%20network%20loss2.jpg)

```
反向传播：从头跑到尾得到预测值，然后使用交叉熵计算出了损失函数，需要计算损失函数对各个参数的导数，来设定下一次新的模型的参数。整个模型的参数有：输出层的f(x)，输出层的a(x)，隐藏层的h(x)，隐藏层的a(x)，每一层与层之间的w和b。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%971.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%972.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%973.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%974.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%975.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%976.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%977.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%978.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B.jpg)

# Gradient Checking
```
f'(x)=f(x)对x的偏导数=(f(x+ε)-f(x-ε))/2ε，ε是很小的数
检验导数求的对不对，用导数公式求得结果与用ε定义法求得结果之差小于10^-6，则认为这步导数求的正确。
```

# Deep Learning Optimization
```
深度学习是非凸函数，求的最优是局部最优而不是全局最优。
Plateau函数形状：先下降，然后很长一段平滑区，然后再下降。
如果常规的梯度下降，可能到平滑区就结束了，不会得到后面更优化的结果。
这种模型一般使用momentum方法来解决。
```

# SGD with Converge
```
先使用固定随机梯度下降的步长η
然后使用动态步长，η需要满足从第一个步长到第无穷个步长的和等于无穷，且需要满足从第一个步长到第无穷个步长的步长平方和小于无穷。
η=α/1+σt ，σ是下降常数
η=α/t^σ （0.5<σ<1）
```

# Early Stopping
```
反复拿训练集来回做训练，无脑增加epoch最后一定会过拟合，导致在验证集上的错误率又提高了。
需要监测验证集上错误率的变化，一旦发现过拟合，赶快在最低点把训练停掉，直接得到训练好的模型。
两条曲线之间有gap，一般来说模型越复杂gap越大，逻辑回归的gap小，神经网络的gap大
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Early%20stopping.jpg)

# Recurrent Neural Network RNN
```
处理股票，天气，语音，文本等时序数据，也可以处理不同时长的数据。
最传统的RNN模型Vanilla RNN，直接一步运算得到中间ht的值，然后乘个权值再softmax均一化一下得到分类的结果，然后可以使用交叉熵得到loss
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Vanilla%20RNN.jpg)
```
HMM模型的隐藏层是one-hot向量，比如隐藏层给出的是"动词"这个选项，然后根据概率往下生成一个单词以及下一个单词的词性。
而RNN中间的隐藏层是一个distributed representation向量。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/RNN%20vs%20HMM.jpg)
```
语言模型LM:
一句话S：w1,w2,w3,...,wn  n是S的单词数
p(S)=p(w1,w2,w3,...,wn)=p(w1)*p(W2|w1)*p(w3|w1,w2)*...*p(wn|w1,w2,w3,..,wn-1)

RNN的语言模型类似
输入的是单词的one-hot值，然后乘上一个之前生成的单词向量矩阵，特地扣出这个单词的向量(只有这一条向量)。
然后和Vanilla RNN一样，乘上权重和偏置(在一次循环中权重偏置是不变的，其实是只有一个隐藏单元来回自己循环，展开了才是现在这个一排的样子)。
y^是预测值，在分类情况下是分成每类的概率，真实标签肯定是其中一类就是one-hot的标签，计算两者的交叉熵，得到这一步的loss。当把所有输入循环完，把每一步的loss加起来，得到当前模型的loss
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/RNN%20for%20LM.jpg)

# Vanishing/Exploding Gradient 梯度消失/梯度爆炸
```
同样是反向传播，这次是通过时间，即BPTT(Through Time)
从J4的loss来一步一步往回推，diag是将里面的向量斜着放，别的地方补0构成对角矩阵
求出J对h的偏导
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B81.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B82.jpg)
```
当梯度爆炸的时候，解决方法是设定个阈值，重新更新一下梯度，不要让它太大
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B83.jpg)

# LSTM
```
例句：The writer of the books (is/are)
如果句子太长，可能忘记了前面的内容，就会根据books错选are
LSTM在RNN中间的隐藏层里加了三个门，遗忘门，输入门，输出门，以及许多权重和偏置，最后还是得到ht
其中"○"是element-wise操作，是两个向量每一位对应每一位单独的乘积。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/LSTM.jpg)
```
并且根据任务目的的不同，LSTM进化出了许多其他结构不同的输入输出方式
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/LSTM%E7%BB%93%E6%9E%84.jpg)
```
双向LSTM是先按正常RNN从左往右跑一遍，然后反向输入再反着跑一遍，将两次得到ht直接concatenate拼接起来。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/%E5%8F%8C%E5%90%91LSTM.jpg)
```
GRU减少了LSTM的参数，训练速度加快，效果基本不变。
将三个门减少成了两个门，还是一顿计算得到ht
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/GRU.jpg)