# 神经网络
```
神经网络一个节点可以分成两部分：
Pre-activation: 由上一层每一个节点和权值的乘积再最后加个偏置b得到
Post-activation: 将Pre-的结果嵌套个激活函数g()
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Pre-activation%20Post-activation.jpg)

```
激活函数：
线性激活函数：g(a)=a，没什么实际意义，原样输出。假设每一层的激活函数都是线性激活，合并之后其实只有一层。
sigmoid激活函数：g(a)=1/(1+e^(-a))，将任意值映射成一个(0,1)之间的数。
tanh激活函数：公式如图，将任意值映射成一个(-1,1)之间的数。
relu激活函数：g(a)=max(0,a)，把线性激活函数小于0的部分全部定义成0。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Linear%20activation%20function1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Linear%20activation%20function2.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/sigmoid%20activation.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/tanh%20activation.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/relu%20activation.jpg)

```
单层神经网络：除去输入层和输出层，中间只有1层隐藏层的叫单层神经网络。通过输入层x和权值w以及偏置b计算隐藏层每个a(x)的值，然后使用激活函数求出h(x)=g(a(x))，再乘上输出层的权值以及偏置得到输出节点的Pre-activation值，输出层的Post-activation一般如果是分类问题激活函数就会选择softmax。
多层神经网络：除去输入层和输出层，中间有多少层就叫多少层神经网络。同样的对每个隐藏层节点先用上一层结果和权值偏置求Pre-activation a(x)，然后 使用激活函数求Post-activation h(x)，输出层使用softmax作为激活函数求出每种可能的均一化概率，然后根据求出的概率和实际的标签label求两者的交叉熵作为这次结果的loss。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/single%20layer%20neural%20network.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Multi-layer%20neural%20network.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/Multiple%20output%20neural%20network.jpg)