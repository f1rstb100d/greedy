# One-hot drawbacks
```
one-hot与distributed representation(word2vec的结果)相比：
1. sparse
2. 无法表达semantic similarity
3. 可以表达的容量小，Capacity小，每一位不是0就是1
```

# Skip-gram model
```
当设定窗口大小为2时，CBOW模型是拿Wi-2,Wi-1与Wi+1,Wi+2来预测Wi；而skip-gram是拿Wi(中心词)来预测前面两个和后面两个(Context上下文)。
目标函数是所有构造的目标函数乘积最大，最外面循环每个中心词，里面套一个循环基于当前的中心词的所有上下文单词。然后θ是模型的参数，由U和V组成，U是这个单词在上下文情况中的向量表示，而V是这个单词在中心词情况下的向量表示。使用softmax表示概率，本质是均一化，分母等于所有分子的和。化简之后发现对于每个中心词都需要跑一遍C'也就是所有的词库，优化方法有负采样和层次softmax。
另一种定义是D等于1即后面两个是中心词和上下文，D等于0表示后面的条件两个单词无关系，最大化有关的D=1的条件概率和无关的D=0的条件概率，同样需要遍历所有的负样本。
定义N(W)为随机选择几个以W为中心词的不在上下文窗口中的负样本，近似作为所有词典的负样本。
然后使用随机梯度下降(每遍历一个词更新一边参数)，因为是argmax，所以是+η*偏导数，
Evaluation of word2vec：使用TSNE降到二维然后画图，计算余弦相似度，类比Woman-man=girl-(boy)
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20formulation1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20formulation2.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20formulation3.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20formulation4.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20formulation5.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20negative%20sampling1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20negative%20sampling2.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20negative%20sampling3.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20negative%20sampling4.jpg)

# SGD GBDT Xgboost

# Word2vec skip-gram drawbacks
```
1. 没有考虑上下文，可以使用context-aware word embedding，比如Elmo Bert
2. 窗口长度有限，优化可以使用Language Model，比如RNN LSTM
3. 无法考虑全局，使用全局模型，比如矩阵分解MF
4. 无法有效学习低频词向量，替换方法由subword embedding，即character n-gram
5. 未登录词(OOV：out-of0vocabulary)，即训练集没出现的单词但在测试集出现了，也可以使用subword embedding
6. 严格意义的语序，可以使用语言模型RNN LSTM Transformer
7. embedding向量的可解释性不够，可以将word嵌入到非欧氏空间中
```

# Learning with subword -- FastText
```
有可能有的单词只在测试集中出现而训练集中没有。
法一最简单就是直接忽略掉。
法二是使用subword--character n-gram，即把所有单词都按n个一组进行拆分，然后skip-gram训练出每部分的向量，最后需要哪些单词就重新求和拼接起来。
若n设置为3 to 6，那么(readding)=(^re)+(rea)+...+(ing)+(ng$)
+(^rea)+(read)+...+(ding)+(ing$)
+(^read)+(readi)+...+(ading)+(ding$)
+(^readi)+(readin)+...+(eading)+(ading$)
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/subword%20FastText.jpg)

# NNLM Neural Network Language Model
```
句子：w1 w2 w3 w4 w5 w6
p(w1,w2,w3,w4,w5,w6)=p(w1)*p(w2|w1)*p(w3|w1,w2)*p(w4|w1,w2,w3)*p(w5|w1,w2,w3,w4)*p(w6|w1,w2,w3,w4,w5)
```

# 一词多义--ELmo
```
对于同一个单词在不同句子中的不同意思，有两种解决方法：
法一：最简单的，就是训练出每个单词的三种语义的向量，然后在测试集中遇到这个单词就再去判断到底是什么意思(本质是3选1)
法二：最好的，ELmo模型，训练的时候还是每个单词得到一个基础向量，然后测试的时候根据语义前后单词(语言模型)得到一个动态调整的偏移量，加在一起得到这个单词在这个句子中的实际向量。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/ELmo1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/ELmo2.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/ELmo3.jpg)

# Word embedding的分类
```
矩阵分解MF：构建一个|V|*|V|的矩阵，统计每两个的单词在文档中的出现频率(只要在同一个句子中就算1次，所以是全局的)，然后SVD分解出一个embedding矩阵。
Glove模型是同时考虑了Global和Local的模型，将MF和skip-gram进行了融合。
Gaussian Embedding是将单词映射成为一个高斯分布(向量配上一个置信度)，如果单词出现的次数太少那么置信度就会较低，计算高斯分布的相似度推荐使用KL散度。
非欧氏空间hyperbolic embedding：不像欧氏空间的fruit和apple的映射，hyperbolic空间最中间映射的是food，然后外围一点是fruit，然后再往外面就是apple banana之类的。
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/word%20embedding%20%E5%88%86%E7%B1%BB1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/word%20embedding%20%E5%88%86%E7%B1%BB2.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/word%20embedding%20%E5%88%86%E7%B1%BB3.jpg)