# One-hot drawbacks
```
one-hot与distributed representation(word2vec的结果)相比：
1. sparse
2. 无法表达semantic similarity
3. 可以表达的容量小，Capacity小，每一位不是0就是1
```

# Skip-gram model
```
当设定窗口大小为2时，CBOW模型是拿Wi-2,Wi-1与Wi+1,Wi+2来预测Wi；而skip-gram是拿Wi(中心词)来预测前面两个和后面两个(Context上下文)。
目标函数是所有构造的目标函数乘积最大，最外面循环每个中心词，里面套一个循环基于当前的中心词的所有上下文单词。然后θ是模型的参数，由U和V组成，U是这个单词在上下文情况中的向量表示，而V是这个单词在中心词情况下的向量表示。使用softmax表示概率，本质是均一化，分母等于所有分子的和。化简之后发现对于每个中心词都需要跑一遍C'也就是所有的词库，优化方法有负采样和层次softmax。
另一种定义是D等于1即后面两个是中心词和上下文，D等于0表示后面的条件两个单词无关系，最大化有关的D=1的条件概率和无关的D=0的条件概率，同样需要遍历所有的负样本。
定义N(W)为随机选择几个以W为中心词的不在上下文窗口中的负样本，近似作为所有词典的负样本。
然后使用随机梯度下降(每遍历一个词更新一边参数)，因为是argmax，所以是+η*偏导数，
Evaluation of word2vec：使用TSNE降到二维然后画图，计算余弦相似度，类比Woman-man=girl-(boy)
```
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20formulation1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20formulation2.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20formulation3.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20formulation4.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20formulation5.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20negative%20sampling1.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20negative%20sampling2.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20negative%20sampling3.jpg)
![](https://github.com/f1rstb100d/greedy/blob/master/jpg/skip-gram%20negative%20sampling4.jpg)